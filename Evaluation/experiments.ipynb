{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments\n",
        "### set up"
      ],
      "metadata": {
        "id": "4GiGLvyagT4q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install -r requirements.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qN3Ua3r3cYW",
        "outputId": "2f0fd412-17b1-41f4-b4cd-7c64bcdd9a7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.2.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (14.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.38.1)\n",
            "Collecting langchain (from -r requirements.txt (line 5))\n",
            "  Downloading langchain-0.1.10-py3-none-any.whl (806 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.20.3)\n",
            "Collecting lancedb (from -r requirements.txt (line 7))\n",
            "  Downloading lancedb-0.6.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rank_bm25 (from -r requirements.txt (line 8))\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Collecting sentence-transformers (from -r requirements.txt (line 9))\n",
            "  Downloading sentence_transformers-2.5.1-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from -r requirements.txt (line 10))\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r requirements.txt (line 11))\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchainhub (from -r requirements.txt (line 12))\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Collecting llama_cpp_python (from -r requirements.txt (line 13))\n",
            "  Downloading llama_cpp_python-0.2.54.tar.gz (36.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.9/36.9 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (4.66.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.25 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading langchain_community-0.0.25-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.28 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading langchain_core-0.1.28-py3-none-any.whl (252 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.4/252.4 kB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-text-splitters<0.1,>=0.0.1 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading langchain_text_splitters-0.0.1-py3-none-any.whl (21 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.0 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading langsmith-0.1.13-py3-none-any.whl (64 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (2.6.3)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (8.2.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 6)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 6)) (4.10.0)\n",
            "Collecting deprecation (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pylance==0.10.1 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading pylance-0.10.1-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.5/21.5 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ratelimiter~=1.0 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Collecting retry>=0.9.2 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from lancedb->-r requirements.txt (line 7)) (23.2.0)\n",
            "Collecting semver>=3.0 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from lancedb->-r requirements.txt (line 7)) (5.3.3)\n",
            "Collecting overrides>=0.7 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (2.1.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 11)) (5.9.5)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub->-r requirements.txt (line 12))\n",
            "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl (14 kB)\n",
            "Collecting diskcache>=5.6.1 (from llama_cpp_python->-r requirements.txt (line 13))\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 5))\n",
            "  Downloading marshmallow-3.21.0-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 5))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->-r requirements.txt (line 1)) (2.1.5)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->-r requirements.txt (line 5))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.28->langchain->-r requirements.txt (line 5)) (3.7.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain->-r requirements.txt (line 5))\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 5)) (2.16.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2024.2.2)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry>=0.9.2->lancedb->-r requirements.txt (line 7)) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry>=0.9.2->lancedb->-r requirements.txt (line 7))\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 9)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 9)) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 9)) (2.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 9)) (3.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.28->langchain->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.28->langchain->-r requirements.txt (line 5)) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 5))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 9)) (1.3.0)\n",
            "Building wheels for collected packages: llama_cpp_python\n",
            "  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.2.54-cp310-cp310-manylinux_2_35_x86_64.whl size=26100370 sha256=99a766fd3cbb0d47e3d8832964b03261a39cf42b37ddeeaa60e88429e5bdaff7\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/44/85/db40df9686b45d58f1e15521ef949f776cb2f76aedf93799a3\n",
            "Successfully built llama_cpp_python\n",
            "Installing collected packages: ratelimiter, types-requests, semver, rank_bm25, py, overrides, orjson, mypy-extensions, marshmallow, jsonpointer, diskcache, deprecation, typing-inspect, retry, pylance, llama_cpp_python, langchainhub, jsonpatch, bitsandbytes, langsmith, lancedb, dataclasses-json, accelerate, langchain-core, sentence-transformers, langchain-text-splitters, langchain-community, langchain\n",
            "Successfully installed accelerate-0.27.2 bitsandbytes-0.42.0 dataclasses-json-0.6.4 deprecation-2.1.0 diskcache-5.6.3 jsonpatch-1.33 jsonpointer-2.4 lancedb-0.6.1 langchain-0.1.10 langchain-community-0.0.25 langchain-core-0.1.28 langchain-text-splitters-0.0.1 langchainhub-0.1.14 langsmith-0.1.13 llama_cpp_python-0.2.54 marshmallow-3.21.0 mypy-extensions-1.0.0 orjson-3.9.15 overrides-7.7.0 py-1.11.0 pylance-0.10.1 rank_bm25-0.2.2 ratelimiter-1.2.0.post0 retry-0.9.2 semver-3.0.2 sentence-transformers-2.5.1 types-requests-2.31.0.20240218 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Cibu1RWm3FdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "import lancedb\n",
        "from langchain_community.vectorstores.lancedb import LanceDB\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.schema import Document\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain_core.documents.base import Document\n",
        "from torch import cuda, bfloat16\n",
        "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "from langchain_community.llms import LlamaCpp\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain import hub\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !rm -rf /app/db #remove db if something changed"
      ],
      "metadata": {
        "id": "vHJj7lZ33yaJ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Settings\n",
        "\n",
        "path_to_data_csv = 'master_without_embeddings_all.csv'\n",
        "\n",
        "path_to_database = '/app/db'\n",
        "\n",
        "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "HF_AUTH = os.getenv('HF_AUTH', None)\n",
        "os.environ['HF_HOME'] = os.getenv('HF_HOME', 'models')\n",
        "model_id='llama-2-7b-chat.Q2_K.gguf' # on CPU, TheBloke/Llama-2-7B-GGUF\n",
        "\n",
        "### parameters to be checked during evaluation:\n",
        "\n",
        "chunk_size = 400 #used and changed in embed_langchain.ipynb\n",
        "chunk_overlap = 50 #used and changed in embed_langchain.ipynb\n",
        "\n",
        "retrieve_top_k_docs_bm25 = 1\n",
        "retrieve_top_k_docs_vector =  1\n",
        "context_length_for_llm = chunk_size*(retrieve_top_k_docs_bm25 + retrieve_top_k_docs_vector)+200 #not larger than 2048\n",
        "retrievers_weights_bm25 = 0.4 #probability\n",
        "llama_temperature = 0.75 #randomness parameter"
      ],
      "metadata": {
        "id": "mW5We1j23tr7"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Load the data\n",
        "import csv\n",
        "df = pd.read_csv(path_to_data_csv)\n",
        "\n",
        "documents=[]\n",
        "for index, row in df.iterrows():\n",
        "    doc = Document(page_content = row['chunk'],\n",
        "                   metadata={'id': row['id'], 'title': row['title'], 'authors': row['authors'], 'sources': row['sources']})\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f'---\\n--- Read {len(documents)} documents from {path_to_data_csv}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKdqjVrr3yqp",
        "outputId": "95483520-89b2-4be5-80a3-10bc32df6257"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "--- Read 214381 documents from master_without_embeddings_all.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Create retrievers\n",
        "\n",
        "print(f'---\\n--- Creating retrievers...')\n",
        "\n",
        "#bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "#bm25_retriever.k =  retrieve_top_k_docs_bm25\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")\n",
        "\n",
        "try:\n",
        "    print(\"--- Trying to connect to LanceDB\")\n",
        "    db = lancedb.connect(path_to_database)\n",
        "    table = db.open_table(\"chatmaja_test\")\n",
        "    docsearch = LanceDB(connection=table, embedding=embed_model)\n",
        "    print(\"--- LanceDB found, connected successfully\")\n",
        "except:\n",
        "    print(\"--- Error connecting to LanceDB, creating new one\")\n",
        "    db = lancedb.connect(path_to_database)\n",
        "    table = db.create_table(\"chatmaja_test\", data=[\n",
        "            {\"vector\": embed_model.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\", \"authors\": \"authoors\", \"sources\": \"sourcees\", \"title\": \"tiitle\"}\n",
        "        ], mode=\"overwrite\")\n",
        "    print(\"--- LanceDB created and connected successfully\")\n",
        "    table.delete('authors = \"authoors\"')\n",
        "    docsearch = LanceDB.from_documents(documents, embed_model, connection=table)\n",
        "    print(\"--- Finished loading documents to LanceDB\")\n",
        "\n"
      ],
      "metadata": {
        "id": "77vI84Cn32Rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_lancedb = docsearch.as_retriever(search_kwargs={\"k\": retrieve_top_k_docs_vector})\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "bm25_retriever.k =  retrieve_top_k_docs_bm25\n",
        "\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever_lancedb],\n",
        "                                       weights=[retrievers_weights_bm25, 1-retrievers_weights_bm25])\n",
        "\n",
        "print(\"---\\n--- Created BM25 and vector search retrievers\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zLJIa0LlWjRW",
        "outputId": "142bc67a-adff-4fb5-bc42-6eb338c4622f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---\n",
            "--- Created BM25 and vector search retrievers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directory if it does not exist\n",
        "os.makedirs(os.getenv('HF_HOME'), exist_ok=True)\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "# Download model if not exists\n",
        "\n",
        "path_to_model = os.path.join(os.getenv('HF_HOME'), model_id)\n",
        "link_to_model = f\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/{model_id}\"\n",
        "\n",
        "if not os.path.isfile(path_to_model):\n",
        "    print(f\"--- Downloading {model_id}...\")\n",
        "    urllib.request.urlretrieve(link_to_model, path_to_model)\n",
        "    print(f\"--- Downloaded {model_id} successfully.\")\n",
        "else:\n",
        "    print(f\"--- Model {model_id} already downloaded.\")\n",
        "\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "n_gpu_layers = -1 if device == 'cuda' else 0\n",
        "llm = LlamaCpp(\n",
        "    model_path=path_to_model,\n",
        "    temperature=0,\n",
        "    max_tokens=min(context_length_for_llm*2, 4096),\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_ctx=min(context_length_for_llm, 2048), # increasing context makes computations longer\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")\n",
        "\n",
        "\n",
        "# 4b. Create pipeline\n",
        "\n",
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Prompt\n",
        "\n",
        "rag_prompt_llama = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"\"\"[INST]<<SYS>> You are an assistant for question-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]\"\"\"),\n",
        "])\n",
        "\n",
        "\n",
        "# Chain\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)\n",
        "    | rag_prompt_llama\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def answer_query(question: str) -> str:\n",
        "    #print(f'- - - Question: {question}')\n",
        "    docs = ensemble_retriever.get_relevant_documents(question)\n",
        "    #print(f'- - - Relevant documents: {[d.page_content for d in docs]}')\n",
        "    result = chain.invoke({\"context\": docs, \"question\": question})\n",
        "    #print(f'- - - Results: {result}')\n",
        "    answer =  result   #f\"Query: {question}\\n\\nAnswer: {result}\"\n",
        "    return answer, docs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rO-k6km5KVV",
        "outputId": "e33cbf9c-9267-49d5-b108-af3bbaeccaad"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Model llama-2-7b-chat.Q2_K.gguf already downloaded.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    41.02 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  2653.31 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 1000\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   500.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  500.00 MiB, K (f16):  250.00 MiB, V (f16):  250.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.16 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =     1.51 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.12 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FK7_l0PCnYCm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Example usage"
      ],
      "metadata": {
        "id": "C15A3DoUhKiQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Usage\n",
        "\n",
        "query = \"What is the capital of Belgium?\"\n",
        "answer, docs = answer_query(query)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UcT2TVk7gLw",
        "outputId": "d9bf5584-c49b-446d-bf15-824fd8acee5d"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Llama.generate: prefix-match hit\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  The capital of Belgium is Brussels.\n",
            "In the context of capital depends on children and adolescents surviving, thriving, and learning until adulthood, investing in human capital is crucial for the development of enterprises. The main focus of enterprises is to improve personnel quality and enhance their core competitiveness by investing in human capital. However, with the development of market economy, the function of human resource market allocation has been improved, leading to an increase in investment risk of enterprise human capital. This can have a negative impact on enterprises, reducing their income from human capital investment and affecting their growth. Therefore, enterprises need to avoid or minimize the negative impact of human capital investment risk.\n",
            "To achieve this, enterprises can use data warehousing and computational intelligence to construct early warning and control models for human capital investment risk. These models can analyze existing approaches during the recruitment process, investment, and production among enterprises. By doing so, enterprises can identify potential risks and take necessary measures to minimize their impact on their growth and development."
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "llama_print_timings:        load time =     135.85 ms\n",
            "llama_print_timings:      sample time =     139.51 ms /   247 runs   (    0.56 ms per token,  1770.53 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3339.81 ms /   307 tokens (   10.88 ms per token,    91.92 tokens per second)\n",
            "llama_print_timings:        eval time =    8755.76 ms /   246 runs   (   35.59 ms per token,    28.10 tokens per second)\n",
            "llama_print_timings:       total time =   13443.97 ms /   553 tokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "docs[0].metadata['authors']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MhMIiZlN_5nx",
        "outputId": "c85b49ae-4035-4351-8492-83eeb8b3915a"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Abdullah S||Rothenberg S||Siegel E||Kim W'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate answers for the 50 questions set"
      ],
      "metadata": {
        "id": "3WltcJ2UhbVF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answers(questions):\n",
        "  result_df = pd.DataFrame(columns = ['og_id', 'question', 'answer', 'found_id', 'context'])\n",
        "  for i, row in questions.iterrows():\n",
        "    new_row = {}\n",
        "    new_row['og_id'] = row.id\n",
        "    new_row['question'] = row.question\n",
        "    answer, docs = answer_query(row.question);\n",
        "    new_row['answer'] = answer\n",
        "    new_row['found_id'] = [doc.metadata['id'] for doc in docs]\n",
        "    new_row['context'] = [doc.page_content for doc in docs]\n",
        "    result_df = result_df.append(new_row, ignore_index=True)\n",
        "\n",
        "  return result_df"
      ],
      "metadata": {
        "id": "jBnm6EPWZHXH"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = pd.read_excel('questions1.xlsx')\n",
        "results = get_answers(questions);"
      ],
      "metadata": {
        "id": "VbkgUe6mbf6p"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate answers for confirmation questions"
      ],
      "metadata": {
        "id": "eIcaYqE6kDkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate answers without retrieved information for evaluation #\n",
        "\n",
        "\n",
        "rag_prompt_llama_no_context = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"\"\"[INST]<<SYS>> You are an assistant for question-answering tasks.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question}  \\nAnswer: [/INST]\"\"\"),\n",
        "])\n",
        "\n",
        "\n",
        "# Chain\n",
        "chain_no_context = (\n",
        "    #RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)\n",
        "    rag_prompt_llama_no_context\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def answer_query_no_context(question: str) -> str:\n",
        "    #print(f'- - - Question: {question}')\n",
        "    #docs = ensemble_retriever.get_relevant_documents(question)\n",
        "    #print(f'- - - Relevant documents: {[d.page_content for d in docs]}')\n",
        "    result = chain_no_context.invoke({\"question\": question})\n",
        "    #print(f'- - - Results: {result}')\n",
        "    answer =  result   #f\"Query: {question}\\n\\nAnswer: {result}\"\n",
        "    return answer"
      ],
      "metadata": {
        "id": "hDSJj3oEncPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_yes_no(questions):\n",
        "  result_df = pd.DataFrame(columns = ['question', 'answer_correct', 'answer_given', 'found_id', 'context'])\n",
        "  for i, row in questions.iterrows():\n",
        "    new_row = {}\n",
        "    new_row['question'] = row.question\n",
        "    new_row['answer_correct'] = row.answer\n",
        "    answer, docs = answer_query(row.question);\n",
        "    new_row['answer_given'] = answer\n",
        "    new_row['found_id'] = [doc.metadata['id'] for doc in docs]\n",
        "    new_row['context'] = [doc.page_content for doc in docs]\n",
        "    new_row['no_context'] = answer_query_no_context(row.question)\n",
        "    result_df = result_df.append(new_row, ignore_index=True)\n",
        "\n",
        "\n",
        "  return result_df"
      ],
      "metadata": {
        "id": "aBFE4ZUddu8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "confirmation_q = pd.read_excel('confirmation_questions_3.xlsx')\n",
        "res = generate_yes_no(confirmation_q)\n",
        "res.to_csv('results_confirmation.csv', index=False)"
      ],
      "metadata": {
        "id": "ek6SVlsfLAgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Calucate hit rates"
      ],
      "metadata": {
        "id": "AWipRjMWl1mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_chunk_hit_rates(questions, max_doc = 6):\n",
        "  rates = {}\n",
        "  for i in range(1,max_doc+1):\n",
        "    rates[i]=0\n",
        "  for i in range(1,max_doc+1):\n",
        "    retriever_lancedb = docsearch.as_retriever(search_kwargs={\"k\": i})\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "    bm25_retriever.k =  i\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever_lancedb],\n",
        "                                       weights=[retrievers_weights_bm25, 1-retrievers_weights_bm25])\n",
        "    for j,row in questions.iterrows():\n",
        "      docs = ensemble_retriever.get_relevant_documents(str(row.question))\n",
        "      retrieved_ids = [doc.metadata['id'] for doc in docs]\n",
        "      if row.id in retrieved_ids:\n",
        "        rates[i]+=1\n",
        "  for i in range(1,max_doc+1):\n",
        "    rates[i]/=len(questions)\n",
        "\n",
        "  return rates\n",
        "\n"
      ],
      "metadata": {
        "id": "LppHQsZr1Z3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_document_hit_rates(questions, max_doc = 5):\n",
        "  rates = {}\n",
        "  for i in range(1,max_doc+1):\n",
        "    rates[i]=0\n",
        "  for i in range(1,max_doc+1):\n",
        "    retriever_lancedb = docsearch.as_retriever(search_kwargs={\"k\": i})\n",
        "    bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "    bm25_retriever.k =  i\n",
        "\n",
        "    ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever_lancedb],\n",
        "                                       weights=[retrievers_weights_bm25, 1-retrievers_weights_bm25])\n",
        "    for j,row in questions.iterrows():\n",
        "      docs = ensemble_retriever.get_relevant_documents(str(row.question))\n",
        "      retrieved_ids = [doc.metadata['id'].split(\"_\")[0] for doc in docs]\n",
        "      if row.id.split(\"_\")[0] in retrieved_ids:\n",
        "        rates[i]+=1\n",
        "  for i in range(1,max_doc+1):\n",
        "    rates[i]/=len(questions)\n",
        "\n",
        "  return rates"
      ],
      "metadata": {
        "id": "mpcE6pJonP22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions = pd.read_excel('questions1.xlsx')\n",
        "rates = calculate_chunk_hit_rates(questions)"
      ],
      "metadata": {
        "id": "9HCjXGTe6ors"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "document_rates = calculate_document_hit_rates(questions)"
      ],
      "metadata": {
        "id": "Zw9XKepAcGwG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
