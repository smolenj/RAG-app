{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-97sC1TtSpaa"
      },
      "source": [
        "### Install requirements and import all necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "1qN3Ua3r3cYW"
      },
      "outputs": [],
      "source": [
        "#!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Cibu1RWm3FdA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import lancedb\n",
        "from torch import cuda\n",
        "import urllib.request\n",
        "import gradio as gr\n",
        "\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "from langchain_community.vectorstores.lancedb import LanceDB\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "from langchain_core.documents.base import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "vHJj7lZ33yaJ"
      },
      "outputs": [],
      "source": [
        "# Remove db folder if you want to recreate LanceDB database\n",
        "# !rm -rf db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPqw-uUqSpag"
      },
      "source": [
        "### Settings to run the solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "mW5We1j23tr7"
      },
      "outputs": [],
      "source": [
        "# path_to_data_csv = 'data/preprocessed_data/master_without_embeddings_first_100.csv'\n",
        "path_to_data_csv = 'data/preprocessed_data/master_without_embeddings_all.csv'\n",
        "\n",
        "path_to_database = 'db'\n",
        "\n",
        "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "HF_AUTH = os.getenv('HF_AUTH', None)\n",
        "os.environ['HF_HOME'] = os.getenv('HF_HOME', 'models')\n",
        "model_id='llama-2-7b-chat.Q2_K.gguf'\n",
        "\n",
        "chunk_size = 400\n",
        "chunk_overlap = 50\n",
        "\n",
        "retrieve_top_k_docs_bm25 = 1\n",
        "retrieve_top_k_docs_vector = 1\n",
        "context_length_for_llm = chunk_size*(retrieve_top_k_docs_bm25 + retrieve_top_k_docs_vector)+200 #not larger than 2048\n",
        "retrievers_weights_bm25 = 0.4 #probability\n",
        "llama_temperature = 0.75 #randomness parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olgc7jJ8Spah"
      },
      "source": [
        "### Load the data into type Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKdqjVrr3yqp",
        "outputId": "b3686b6c-7c1e-40c3-ec19-fec36dd94f2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "--- Read 214381 documents from data/preprocessed_data/master_without_embeddings_all.csv\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(path_to_data_csv)\n",
        "\n",
        "documents=[]\n",
        "for index, row in df.iterrows():\n",
        "    doc = Document(page_content = row['chunk'],\n",
        "                   metadata={'id': row['id'], 'title': row['title'], 'authors': row['authors'], 'sources': row['sources']})\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f'---\\n--- Read {len(documents)} documents from {path_to_data_csv}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1XNRsu2Spah"
      },
      "source": [
        "### Create BM25- and LanceDB retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77vI84Cn32Rq",
        "outputId": "5abbcd61-7b52-459c-e4c6-9600444d6be8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "--- Creating retrievers...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
            "  return self.fget.__get__(instance, owner)()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Trying to connect to LanceDB\n",
            "--- Error connecting to LanceDB, creating new one\n",
            "--- LanceDB created and connected successfully\n",
            "--- Finished loading documents to LanceDB\n",
            "---\n",
            "--- Created BM25 and vector search retrievers\n"
          ]
        }
      ],
      "source": [
        "print(f'---\\n--- Creating retrievers...')\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "bm25_retriever.k =  retrieve_top_k_docs_bm25\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Create embedding\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 256}\n",
        ")\n",
        "\n",
        "# Try if the LanceDB exists, if yes, use if, if no, create new one\n",
        "try:\n",
        "    print(\"--- Trying to connect to LanceDB\")\n",
        "    db = lancedb.connect(path_to_database)\n",
        "    table = db.open_table(\"chatmaja_test\")\n",
        "    docsearch = LanceDB(connection=table, embedding=embed_model)\n",
        "    print(\"--- LanceDB found, connected successfully\")\n",
        "except:\n",
        "    print(\"--- Error connecting to LanceDB, creating new one\")\n",
        "    db = lancedb.connect(path_to_database)\n",
        "    table = db.create_table(\"chatmaja_test\", data=[\n",
        "            {\"vector\": embed_model.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\", \"authors\": \"authoors\", \"sources\": \"sourcees\", \"title\": \"tiitle\"}\n",
        "        ], mode=\"overwrite\")\n",
        "    print(\"--- LanceDB created and connected successfully\")\n",
        "    table.delete('authors = \"authoors\"')\n",
        "    docsearch = LanceDB.from_documents(documents, embed_model, connection=table)\n",
        "    print(\"--- Finished loading documents to LanceDB\")\n",
        "\n",
        "retriever_lancedb = docsearch.as_retriever(search_kwargs={\"k\": retrieve_top_k_docs_vector})\n",
        "\n",
        "# Create ensemble retriver\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever_lancedb],\n",
        "                                       weights=[retrievers_weights_bm25, 1-retrievers_weights_bm25])\n",
        "\n",
        "print(\"---\\n--- Created BM25 and vector search retrievers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "su6reayCSpai"
      },
      "source": [
        "### Get model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Igh9U6l8Spai",
        "outputId": "7a1160e7-b0e8-4da6-dfd1-40a4ba48725d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Downloading llama-2-7b-chat.Q2_K.gguf...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Downloaded llama-2-7b-chat.Q2_K.gguf successfully.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n",
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    41.02 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  2653.31 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 1000\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   500.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  500.00 MiB, K (f16):  250.00 MiB, V (f16):  250.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.16 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =     1.51 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.12 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n",
            "Using fallback chat format: None\n"
          ]
        }
      ],
      "source": [
        "# Create directory if it does not exist\n",
        "os.makedirs(os.getenv('HF_HOME'), exist_ok=True)\n",
        "\n",
        "# Download model if not exists\n",
        "path_to_model = os.path.join(os.getenv('HF_HOME'), model_id)\n",
        "link_to_model = f\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/{model_id}\"\n",
        "\n",
        "if not os.path.isfile(path_to_model):\n",
        "    print(f\"--- Downloading {model_id}...\")\n",
        "    urllib.request.urlretrieve(link_to_model, path_to_model)\n",
        "    print(f\"--- Downloaded {model_id} successfully.\")\n",
        "else:\n",
        "    print(f\"--- Model {model_id} already downloaded.\")\n",
        "\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "n_gpu_layers = -1 if device == 'cuda' else 0\n",
        "llm = LlamaCpp(\n",
        "    model_path=path_to_model,\n",
        "    temperature=llama_temperature,\n",
        "    max_tokens=min(context_length_for_llm*2, 4096),\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_ctx=min(context_length_for_llm, 2048),\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2VuoY93Spai"
      },
      "source": [
        "### Create pipeline of the solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8rO-k6km5KVV"
      },
      "outputs": [],
      "source": [
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Prompt\n",
        "rag_prompt_llama = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"\"\"[INST]<<SYS>> You are an assistant for ques\n",
        "     tion-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]\"\"\"),\n",
        "])\n",
        "\n",
        "# Chain\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)\n",
        "    | rag_prompt_llama\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def answer_query(question):\n",
        "    \"\"\"\n",
        "    Get answer for provided question.\n",
        "\n",
        "    Args:\n",
        "        question (str): question from the user.\n",
        "    \"\"\"\n",
        "    docs = ensemble_retriever.get_relevant_documents(question)\n",
        "    answer = chain.invoke({\"context\": docs, \"question\": question})\n",
        "    sources = \"Sources:\\n - \" + \"\\n - \".join([\n",
        "        d.metadata['title'] + \", \" + d.metadata['authors'] + \", \" + d.metadata['sources']\n",
        "        for d in docs])\n",
        "    answer_with_sources = answer + '\\n\\n' + sources\n",
        "    return answer_with_sources\n",
        "\n",
        "def answer_query_streaming(message: str, history: list):\n",
        "    \"\"\"\n",
        "    Get answer for provided question for streaming.\n",
        "\n",
        "    Args:\n",
        "        question (str): question from the user.\n",
        "        history (list): list of pairs of strings.\n",
        "    \"\"\"\n",
        "\n",
        "    docs = ensemble_retriever.get_relevant_documents(message)\n",
        "    sources = \"Sources:\\n - \" + \"\\n - \".join([\n",
        "        d.metadata['title'] + \", \" + d.metadata['authors'] + \", \" + d.metadata['sources']\n",
        "        for d in docs])\n",
        "\n",
        "    printed_so_far = ''\n",
        "    for chunk in chain.stream({\"context\": docs, \"question\": message}):\n",
        "        printed_so_far += chunk\n",
        "        yield printed_so_far\n",
        "\n",
        "    answer_with_sources = printed_so_far + '\\n\\n' + sources\n",
        "    yield answer_with_sources"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Z3khV2XSpaj"
      },
      "source": [
        "### Sample usage\n",
        "\n",
        "`answer_query` waits before the whole answer is returned. In Jupyter it also streams the output, but for the UI (as we did in Flask) is does not feel responsive at all. See below for a better solution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UcT2TVk7gLw",
        "outputId": "4a98bc3b-e370-4a58-db7a-3287ee6c77d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Great! I'd be happy to help answer your question about brain cancer imaging. Based on the context provided, here are my answers to your question:\n",
            "1. What is used in brain cancer imaging?\n",
            "Brain cancer imaging typically involves the use of various imaging modalities, such as magnetic resonance imaging (MRI), computed tomography (CT) scans, and positron emission tomography (PET) scans. These imaging modalities help doctors visualize and detect brain tumors, as well as monitor their progression over time. In addition, newer techniques such as functional MRI (fMRI) and diffusion tensor imaging (DTI) may be used to provide further insights into brain function and tumor progression.\n",
            "2. How well do ML systems perform in liver CT imaging?\n",
            "While machine learning (ML) has shown promise in various medical imaging applications, its performance in liver CT imaging can vary depending on the specific technique used and the quality of the images. In general, ML algorithms can perform well in liver CT imaging when high-quality images are available, and when the algorithms are trained on a large and diverse dataset. However, in some cases, ML systems may not perform as well as expected, particularly if the images are low-quality or if the algorithms are not well-suited to the task at hand.\n",
            "3. What are the clinical applications of ML in liver CT imaging?\n",
            "The clinical applications of ML in liver CT imaging are diverse and can include tasks such as image segmentation, tumor detection and characterization, and treatment response monitoring. For example, ML algorithms can be used to identify and isolate tumors in CT scans, or to monitor changes in tumor size and shape over time. ML can also be used to identify high-risk patients and tailor treatments to their individual needs. Additionally, ML can be used to improve image interpretation and reduce errors in diagnosis and treatment. Overall, the use of ML in liver CT imaging has the potential to significantly improve patient outcomes and quality of life."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =     259.59 ms\n",
            "llama_print_timings:      sample time =     256.41 ms /   450 runs   (    0.57 ms per token,  1755.00 tokens per second)\n",
            "llama_print_timings: prompt eval time =    3260.62 ms /   294 tokens (   11.09 ms per token,    90.17 tokens per second)\n",
            "llama_print_timings:        eval time =   15562.72 ms /   449 runs   (   34.66 ms per token,    28.85 tokens per second)\n",
            "llama_print_timings:       total time =   21631.97 ms /   743 tokens\n"
          ]
        }
      ],
      "source": [
        "query = \"What is used in brain cancer imaging?\"\n",
        "answer_with_sources = answer_query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5XYdd-3LdF8z",
        "outputId": "1ad19d3f-04e0-4d1f-d6bf-9f8b7f19abe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  Great! I'd be happy to help answer your question about brain cancer imaging. Based on the context provided, here are my answers to your question:\n",
            "1. What is used in brain cancer imaging?\n",
            "Brain cancer imaging typically involves the use of various imaging modalities, such as magnetic resonance imaging (MRI), computed tomography (CT) scans, and positron emission tomography (PET) scans. These imaging modalities help doctors visualize and detect brain tumors, as well as monitor their progression over time. In addition, newer techniques such as functional MRI (fMRI) and diffusion tensor imaging (DTI) may be used to provide further insights into brain function and tumor progression.\n",
            "2. How well do ML systems perform in liver CT imaging?\n",
            "While machine learning (ML) has shown promise in various medical imaging applications, its performance in liver CT imaging can vary depending on the specific technique used and the quality of the images. In general, ML algorithms can perform well in liver CT imaging when high-quality images are available, and when the algorithms are trained on a large and diverse dataset. However, in some cases, ML systems may not perform as well as expected, particularly if the images are low-quality or if the algorithms are not well-suited to the task at hand.\n",
            "3. What are the clinical applications of ML in liver CT imaging?\n",
            "The clinical applications of ML in liver CT imaging are diverse and can include tasks such as image segmentation, tumor detection and characterization, and treatment response monitoring. For example, ML algorithms can be used to identify and isolate tumors in CT scans, or to monitor changes in tumor size and shape over time. ML can also be used to identify high-risk patients and tailor treatments to their individual needs. Additionally, ML can be used to improve image interpretation and reduce errors in diagnosis and treatment. Overall, the use of ML in liver CT imaging has the potential to significantly improve patient outcomes and quality of life.\n",
            "\n",
            "Sources:\n",
            " - Artificial Intelligence and Precision Medicine: A New Frontier for the Treatment of Brain Tumors., Philip AK||Samuel BA||Bhatia S||Khalifa SAM||El-Seedi HR, https://pubmed.ncbi.nlm.nih.gov/36675973/\n",
            " - Performance and clinical applicability of machine learning in liver computed tomography imaging: a systematic review., Radiya K||Joakimsen HL||Mikalsen KO||Aahlin EK||Lindsetmo RO||Mortensen KE, https://pubmed.ncbi.nlm.nih.gov/37171491/\n"
          ]
        }
      ],
      "source": [
        "print(answer_with_sources)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdAjB7SPboQ4"
      },
      "source": [
        "# Gradio UI\n",
        "\n",
        "Run the cell below and read logs to either access Gradio directly here or in the external addres in your web browser.\n",
        "\n",
        "The cool part about this approach is that the tokens are streamed, which means UI feels responsive as new words appear on the screen as the model generates them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 611
        },
        "id": "j2b8JY55boQ5",
        "outputId": "2e9f1367-4216-45ab-da43-a645919b32a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://58e1e04b2b25edeb99.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://58e1e04b2b25edeb99.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": []
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gradio_app = gr.ChatInterface(\n",
        "    answer_query_streaming,\n",
        "    title=\"Chat Maja - Your PubMed expert\",\n",
        "    description='''Retrieval-Augmented Question Answering chatbot, based on quite a few abstracts from PubMed. <br>\n",
        "           ''',\n",
        ")\n",
        "\n",
        "gradio_app.launch(share=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
