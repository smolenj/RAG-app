# ChatMaJA – LLM Based Chatbot for Medical Data


**Heidelberg University**



**Anti-plagiarism Confirmation** We declare that the assignment on the topic mentioned above:
1. Is the result of our independent work.
2. Makes use of no other sources or materials other than those referenced, and that quotations and paraphrases obtained from the work of others are indicated as such.
3. Authors found this report plagiarism-free.

-----------------------

## Abstract
Many AI-powered tools have gained popularity recently, such as ChatGPT, Gemini and Bing. These tools belong to a group called Large Language Models (LLM) and are being used in various fields, including medicine. A question-answering system based on AI can help a large audience determine diseases based on symptoms, summarize massive clinical data and educate themselves with new research in medical technologies. This project aims to create a medical chatbot called ChatMaJA, which provides answers to medical queries. To generate more accurate responses, the chatbot uses data from the PubMed database, which is explained in detail in the report below.

**Keywords:** ChatMaJA; RAG; LanceDB; BM25; LLM; Docker; PubMed

## Introduction

Lately, LLMs gained a lot of trust and popularity. Therefore many are using them to speed up getting answers, especially in domains, where the time is crucial. This is a very promising area of research, as the answers generated by such systems can be fast, accurate, and stylistically correct.

In this article, we introduce chatMaJA, which is a question-and-answering machine, using the Large Language Model to answer questions related to medical data. This chatbot can provide answers to both yes/no and open questions. The answers are based on the LLM model performance and the data we add to the model - in this case, the data source is [PubMed](https://pubmed.ncbi.nlm.nih.gov/). The need for this kind of LLM is because of the huge medical datasets like Clinical notes, medical research papers and electronic health records. Thus there comes the requirement to summarise this data and provide it as query query-based machine to users. General users can educate themselves on the symptoms of various diseases for self-awareness, and doctors can use this chatbot to understand new research and clinical data from other researchers.

In the `Related Work` section, we describe the advantages and disadvantages of key terms and technologies used to create ChatMaJA, in `Methods and Approach` we provide the details about the created pipeline, `Experiments setup` section presents the setup for the experiments on the pipeline, in `Evaluation` we describe metrics we used and the results we got. At the end, we summarize the report in the `Conclusions and future work` section.

## Related Work

Large Language Models [1] can recognize the text and generate a response based on it. The LLMs are built on the concept of machine learning, specifically, Transformers [2] which is a type of neural network. Initially, a lot of data is gathered from the different sources which is then fed to this model for training purposes, that is learning to understand the natural language. The quality of the data fed is proportional to the quality of the model thus initially the data needs to be filtered so that it does not provide wrong information. LLMs use a specific type of machine learning method called deep learning to understand words, sentences, and characters and how they function together. It involves probabilistic analysis of unstructured data which is then used to recognize distinct data by the model.

chatMaJA - the created domain-specific question-answering system - is based on the Retrieval Augmented Generation (RAG) [3] framework. RAG addresses the limitations of pure LLMs due to the fact, that we can provide separate knowledge to the system. In such a system, first, the important information is retrieved from external sources. Only later pure LLM is used to answer the question, with the use of retrieved knowledge. This approach has various advantages including the possibility to add specific knowledge to the LLM, to make the response more accurate. It also allows them to provide better-structured answers written by LLM. This approach also saves a lot of money, due to the fact, that we can use already trained LLM instead of creating our own.

In our project, we used BM25 which is a probabilistic retrieval model [4]. It is a framework that calculates the relevance of a document to a given search query. It is based on the TF-IDF (Term Frequency-Inverse Document Frequency) approach. The purpose of BM25 is to provide a score that indicates how well a document matches the user's search terms. BM25 treats documents as collections of words. It does not consider word order or complex language structure.

Further, we made use of a vector database. At first, we considered [Milvus](https://milvus.io). However, due to not satisfactory results from only a semantic search, we decided to use a hybrid search. However, due to implementation difficulties with hybrid search in Milvus, we decided to change to serverless database LanceDB.

Our application uses language models, therefore we decided to use a framework, which helps in developing such applications - LangChain [5]. The main advantage of this solution is that it provides a similar interface to use various models, retrievers, vector stores, embedding models, and much more. However, the big disadvantage is that not all features in such tools are wrapped with LangChain, in our case the result was that we had to change from Milvus to LanceDB. Due to the advantages and limitations of BM25 and LanceDB described above, we decided to combine those two approaches with EnsembleRetriever from LangChain.

There are various frameworks, that help in creating UI, from which we decided to use [Flask](https://flask.palletsprojects.com/en/3.0.x/) and [Gradio](https://www.gradio.app). The latter allows for a friendly web interface, easily integrated into Jupyter Notebook and therefore live streaming of the answer generated by our solution.

One of the most insightful transformer models ChatGPT potentially demonstrates high performance of pretraining on a large volume of text which is further finetuned to domain-specific problems/patterns. It also explores the profound model Generative Pre-Trained (GPT) Model released as the largest LLM model processing 175 billion parameters [6]. In our project, we decided to use ChatGPT while evaluating our solution.

For the final solution, we decided to use Llama2 [7] 7B model - one of the pre-trained and fine-tuned LLMs created by Meta. 7B in the name means, that it has 7 billion parameters. LLama2 is based on transformer architecture, with changes including the use of different activation function, attention, and pre-normalization. The model is optimized in such a way, that it can easily be used in chat-related tasks.

During our work on the embeddings and models, we used [HuggingFace](https://huggingface.co) platform. Unfortunately, the LLMs are big, which results in using a lot of computational power, even if only to use the model, without training it. The solution that helps in reducing the computational and memory costs is [quantization](https://huggingface.co/docs/optimum/concept_guides/quantization). In our project, we decided to use the model after quantization, which means its weights and activations were saved with lower precision. The big advantage for us was, that we were able to run the model after quantization on our resources and get the answer after a resonable amount of time. The cost of saving the computational power can be a deterioration of the results, but we did not encounter this.

One might incur delusional results on the LLMs output, this unverified output is called Hallucination. [8] These LLMs generate output based on the training data, it is designed to predict the sequence while not inspecting the accuracy of the generated text. Thus, providing wrong outputs that might not be in existence. 

## Methods and Approach

Our system, ChatMaJA, can be accessed via UI where the user can write a question and read the answer, which includes links to relevant documents that the answer was based on.
The following plots show Flask and Gradio UIs.

![Screenshot of Flask UI](/img/Flask_UI.png)

![Screenshot of Gradio](/img/Gradio.png)

From the technical point of view, our solution includes two variants of UI, but both use the same backend system described below. The first front-end was developed in Flask and is easily available by building and running a standalone Docker container. The other UI uses a ready [Gradio template](https://www.gradio.app/guides/creating-a-chatbot-fast) and can be accessed by running a Jupyter Notebook, for example in Google Colab with GPU. The reason for also presenting the second UI is that it showcases the possibility of live-streaming the model's output to the UI. It increases responsiveness and thus greatly improves user experience. Overall, we can run our system in both Jupyter Notebook and Docker.

The created domain-specific question-answering system is based on our Retrieval Augmented Generation framework, which in our case means that the system passes additional data from relevant articles to the LLM when generating an answer. Data, that our system has access to, comes from the medical domain, from PubMed, and we acquired it in the following manner.

**Data preparation**

The process of downloading and preparing data before it is ready to be loaded by our pipeline includes the following steps:

1. Manually download IDs of articles from PubMed (PMIDs) for selected queries and time range:

    Open PubMed -> search for keyword "Intelligence" -> select year range 2013-2023 -> download list of PMIDS.

    Due to PubMed's limitation of the maximum number of IDs downloaded at once, the downloading of all IDs has to be done in parts, for different time ranges. Should there be an overlap in time ranges, deduplication of IDs is also done at this step.

2. Use a website to download `.xlsx` with abstracts and other metadata of articles based on their PubMed ID:

    Go to pubmed2xl.com -> input PMIDs -> download the abstract, title, and all other information for papers with these IDs.

3. Read all `.xlsx` files with [pandas library](https://pandas.pydata.org/docs/), filter columns (choose only columns: `PMID`, `Title`, `Abstract`, `Source`, `Author(s)`), and save as CSV.

4. Read the CSV file, and:

    - split abstracts into chunks - this important step is done leveraging LangChain [RecursiveCharacterTextSplitter](https://api.python.langchain.com/en/latest/text_splitter/langchain.text_splitter.RecursiveCharacterTextSplitter.html), which splits every abstract into chunks in such a way, that every chunk is of length no more than 400 tokens and adjacent chunks have an overlap of 50 tokens. The splitting is done iteratively, first by double new-line, then by single new-line, then by space, and finally by every letter, but only for chunks which (after encoding) are longer than the given limit of 400. Text is encoded into tokens only for computing the length of a chunk,
    - set chunk ID as PMID_chunk_number, 
    - set sources as direct links to given articles in PubMed.

5. Save into a CSV file with columns: `id`, `title`, `chunk`, `authors`, and `sources`.

**Pipeline**

Later, the prepared data is loaded from CSV into a list of LangChain Documents. This type of document is then used to create BM25 and LanceDB LangChain retrievers.

- BM25 retriever - works in memory, we use it as a lexicographical search[3],
- [LanceDB retriever](https://docs.llamaindex.ai/en/stable/examples/vector_stores/LanceDBIndexDemo.html) - this is a vector database with files stored locally in a folder, we use it for semantic search. Its LangChain wrapper does not allow for changing the similarity metrics, we use the default L2.

In LanceDB, we set the embeddings to be computed with the `sentence-transformers/all-MiniLM-L6-v2` model from HuggingFace [[link](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2)]. As can be seen in the leaderboard [here](https://huggingface.co/spaces/mteb/leaderboard), this model is very small (model size 0.09GB) but performs relatively well (position 72 out of 172 at the time of writing).

To allow both semantic and lexicographical search, we use `EnsembleRetriever` from LangChain, which combines the results from BM25 and LanceDB. It compares the similarity scores between the relevant documents returned by both retrievers (with the possibility of putting more weight on one of the retrievers) and makes a single list of them. This allows us to provide a question to the EnsembleRetriever and get relevant documents on the topic. 

Finally, our system is ready to answer questions from the user. For a given query, a list of relevant chunks of abstracts is retrieved (which are LangChain Documents). The prompt is then defined with the following text:

```
[INST]<<SYS>> You are an assistant for question-answering tasks. Use the following pieces of retrieved-context to answer
the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer
concise.<</SYS>> 
Question: {question} 
Context: {context} 
Answer: [/INST]
```

Fields `{question}` and `{context}` are replaced with users' questions, and texts of the chunks respectively.

This prompt is a default prompt from LangChain for RAG with Llama [[link](https://smith.langchain.com/hub/rlm/rag-prompt-llama)]. Although we wrote it in a way to easily write our prompt (to try different introductions etc.), in the end, we decided to not change it.

To make the answer meaningful for the user, we use LLM to generate the answer, based on the relevant documents. For our project, we use the `Llama-2-7B` model from HuggingFace. To allow for local testing without GPU, we limited the model size to the smallest of the already-quantized models and we selected [llama-2-7b.Q2_K.gguf](https://huggingface.co/TheBloke/Llama-2-7B-GGUF/blob/main/llama-2-7b.Q2_K.gguf) which uses 2-bits quantization for parts of the weights, and results in using just 2.5625 bits per weight as explained [[here](https://huggingface.co/TheBloke/Llama-2-7B-GGUF#explanation-of-quantisation-methods)].

Lastly, titles, authors, and links to relevant articles in PubMed (which were retrieved and passed to the model as context) are appended to the answer generated by the LLM. This way, a user can verify if the answer was generated based on the relevant articles, but also easily access the whole article in PubMed to learn more about the given topic.


## Evaluation
### Setup
The evaluate the system we performed both quantitive and qualitative analysis. We wanted to test variety of aspects, including retrieval accuracy, quality of answer or handling unexpected inputs. For those purposes, two test dataset were developed:
* 50_questions - a dataset consisting of 50 questions ranging from confirmation questions to more complex descriptive questions. Each question was based on an individual chunk of text, the id of which is stored in the table. Exemplatory answers are not provided. Located in `Evaluation/experiment results/50_answered.csv` (with added answers and context generated by the model).
* confirmation_questions - A set of 10 yes/no questions with reference correct answers based on the articles in the database was constructed and used to evaluate the accuracy of the RAG system. Located in `Evaluation/experiment results/confirmation_answered.csv` (with added answers and context generated by the model).

All experiments were conducted using Google Colab with T4 GPU. The exact approach including all parameters is described in file `Evaluation/experiments.ipynb`.

### Retrieval Evaluation
Our system uses an ensemble retriever, consisting of a BM25 retriever and a LanceDB retriever. The number of returned chunks for each retriever was set using parameter k. To evaluate the retrieval part of the system, we examined the average hit-rate for the created dataset of 50 questions based on the value of k. Hit-rate was defined as 1 if the original chunk ID was among returned documents and 0 otherwise. The results are displayed in the following plot. We examined both chunk-level hit-rates (exact chunk needs to match) and article-level.
<img src="/img/hit_rates.png" alt="image" width="700" height="auto">

Although the questions were created specifically based on particular chunks, one must remember that because of the size of our database and multiple articles dealing with similar subjects, there are natural limitations to the hit-rate. The most important conclusion from the experiment is that increasing the value of k does not dramatically increase hit-rate - in most cases, the relevant text was found already with k=1 (or not found at all).

### LLM-based evaluation
The usage of LLMs in examining other NLP systems is an interesting idea. It has been explored and proved to be able to correlate well with human judgements. Such an approach has been described in [8].

For experiments, we used the answers generated by our system for the dataset of 50 chunk-based questions. As an LLM, we used ChatGPT3.5 with the following prompt:
```
For each row, generate a score on a scale from 1-5 in the corresponding categories:
1. Overall quality and accuracy of the answer 
2. Accuracy of the found context to the question
3. Diversity of the style of the answer - penalize direct copying from the context
For each row, provide a short reasoning for each score.
```
Results are stored in `Evaluation/experiment results/llm_evaluation.csv`. The average scores and standard deviations were as follows:
|Overall quality | Accuracy of the context | Diversity
|----------------|-------------------------|-----------
|4.78(0.46) | 4.90(0.30) | 4.52(0.64) |

The scores indicate an overall good quality of the system. An important insight is the high accuracy of the context, compared to the rather low hit-rate. It means that often even though the original article was not found, retrieved information is still somehow relevant. However, one must keep in mind, that ChatGPT is also prone to misjudgment/hallucinations, so these results need to be paired with other evaluation techniques.  
### Confirmation question answering
Because the system provided longer answers (even after some additional prompting), the results have been summarized as Yes/No/Can't tell. As a baseline, we compared the results with the answers obtained without the use of any retrieved context. The results were as follows:
| Question | Correct answer | Generated answer | Aswer without context |
|---------|-----------------|-----------------| ---------------------|
|Is melatonin a white pigment?| No | No | No|
|Is fetal echocardiography good at detecting cardiac anomalies?| Yes | Yes| Yes |
|Does gender play a role in a link between alcohol-related morbility and IQ?| No | No | Can' tell |
|Is Mucopolysaccharidosis Type II an X-linked disorder affecting both males and females?|Yes | Yes | Yes
|Is abscence epilepy etiologically homogeneous? | No | Can't tell | Can't tell | 
|Does gray matter contribute to intelligence? | Yes | Yes | Yes|
|Do children with autism fail to read emotion body language? | No | Can't tell | Yes |
|Do obese children experience emotional problems more often? | Yes | Yes | Yes |
|Will AI replace radiologists? | No | No | Can' tell |
|Does muscle mass play role in clinical outcome of patients with liver disease? | Yes | Yes | Yes

ChatMaJA achieved good accuracy (80%) on the test dataset. What is important, it outperformed basic llama model, proving the benefits of including the context in answer generation process. We observed that it struggled with questions with negative answers - it was much better at confirming true statements. 
### Edge cases, limitations
#### Other languages
Tests have proved that the system is unable to handle other languages. The problem comes at least partly from the retrieval part - the documents retrieved are irrelevant and the answer generated based on them is not correct.
#### Non-medical questions
Example:
```
Q: What is the capital of Belgium?
A:   The capital of Belgium is Brussels.
In the context of capital depends on children and adolescents surviving, thriving, and learning until adulthood, investing in human capital is crucial for the development of enterprises. The main focus of enterprises is to improve personnel quality and enhance their core competitiveness by investing in human capital. However, with the development of the market economy, the function of human resource market allocation has been improved, leading to an increase in the investment risk of enterprise human capital. 
```
The system struggles to answer questions that do not refer to the information inside the database. It gets confused by the irrelevant context provided as a part of the RAG pipeline.

#### Questions included in the retrieved context
If the context retrieved by the ensemble retriever contains some questions in it, the model gets confused and tries to answer those questions as well. For example, see case 37976385_1 (row 11) in the `Evaluation/experiment results/50_answered.csv`.

## Conclusions and Future Work
Retrieval Augmented Generation seems to be the future of question-answering systems. Development of such system gave us valuable insight about this approach and what could be performed in the future to improve the accuracy of ChatMaJA:
* By adding context, the system is able to outperform basic blank LLM model. It is especially relevant for highly specific questions.
* Retrieval of relevant information is an important and complex task. Our first approaches (using only vector similarities) failed to find relevant documents in the vast database. Even when using ensemble retriever, it was often difficult to find the exact document, as many dealt with similar topics. Further exploration in this area could improve the system.
* Compared to other areas of Machine Learning, evaluation of NLP systems is a non-trivial task that requires multiple angles and resources.
* Even without context, the model was able to accurately answer a sizable portion of evaluation questions. It indicates that choice of the model is a crucial part of the good solution. However, the problems of computational power and accessibility of the model arise - we believe that we were able to achieve satisfactory results considering those limitations.
* The system is prone to edge cases or improper use - when the input did not consist of a medical question, the model would get confused and generate inaccurate answers. It was caused by the found context, which was irrelevant to the input. Further work could be done to deal with such cases - the model should be able to ignore inaccurate context.
* The information in the database can be outdated/incomplete - by enabling the system to use web searches to find relevant documents, we would be able to retrieve highly specific context to any question. Such an approach is used by models like Google Gemini. However, it was certrainly out of scope for this project.


### Acknowledgments: 
We acknowledge the overall support of our Professor Michael Gertz and Mentor Robin Khanna over the development of this project.

### Funding:
This research received no external funding.

### Conflicts of Interest: 
The authors declare no conflicts of interest.

## References
1. Naveed, H., Khan A., et al. (2024). A Comprehensive Overview of Large Language Models. arXiv. https://arxiv.org/pdf/2307.06435.pdf
2. Vaswani, A., Shazeer, N., et al. (2017). Attention is All you Need. arXiv (Cornell University), 30, 5998–6008. https://arxiv.org/pdf/1706.03762v5
3. Lewis, P., Perez, E. et al. (2020). Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, Advances in Neural Information Processing Systems 2020, 33, 9459-9474. https://proceedings.neurips.cc/paper/2020/hash/6b493230205f780e1bc26945df7481e5-Abstract.html
4. Robertson, S., Zaragoza, H. (2009). The Probabilistic Relevance Framework: BM25 and Beyond. Foundations and Trends in Information Retrieval. 3 (4): 333–389. https://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf
5. Chase, H. (2022). LangChain [Computer software]. https://github.com/langchain-ai/langchain
6. Radford, A., Narasimhan, K., et al. ( 2018). https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf
7. Touvron, H., Martin, L., (2023). Llama 2: Open Foundation and Fine-Tuned Chat Models. Gen AI, Meta. https://arxiv.org/abs/2307.09288
8. Chiang, LeeCan. (2023). Large Language Models Be an Alternative to Human Evaluations?. ACL 2023. https://aclanthology.org/2023.acl-long.870


**Disclaimer/Publisher’s Note: The statements, opinions and data contained in all publications are solely those of the individual author(s) and contributor(s) and not of the editor(s). The editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.**
